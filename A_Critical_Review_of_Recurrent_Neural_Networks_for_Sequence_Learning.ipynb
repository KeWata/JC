{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just load modules that are used later...\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notes for Journal Club 2016/02/23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Title:\t A Critical Review of Recurrent Neural Networks for Sequence Learning\n",
    "* Authors:\tLipton, Zachary C.; Berkowitz, John; Elkan, Charles\n",
    "* Publication Date:\t05/2015\n",
    "* URL: http://arxiv.org/abs/1506.00019\n",
    "* Keywords:\tComputer Science - Learning, Computer Science - Neural and Evolutionary Computing\n",
    "* Abstract\n",
    "> Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main topics\n",
    "* Reccurent Neural Networks (RNNs)\n",
    "    * LSTM (Long short-term memory)\n",
    "    * Bidirectional Neuroal Networks (BRNN)\n",
    "* Distributed Represantation\n",
    "    * Word2Vec\n",
    "* Applications\n",
    "    * Language translation\n",
    "    * Image Capturing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedfoward Neural Network\n",
    "<img src=FNN.png width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reccurent Neural Network\n",
    "<img src=RNN1.png width=800px>\n",
    "\n",
    "A verbose discription\n",
    "<img src=RNN2.png width=800px>\n",
    "xt-1でなくxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\n",
    " \\mathbf{h}^{(t)} = \\sigma(W^{hx}\\mathbf{x}^{(t)} + W^{hh}\\mathbf{h}^{(t-1)}) \\\\\n",
    " \\mathbf{y}^{(t)} = \\mathrm{softmax}(W^{yh}\\mathbf{h}^{(t)})\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "<img src=https://qiita-image-store.s3.amazonaws.com/0/60969/91876b7a-bf2a-5585-a0bf-c6600c6d638e.png width=800px>\n",
    "Retrieved from [Here](http://qiita.com/t_Signull/items/21b82be280b46f467d1b).\n",
    "<img src=LSTM.png width=800px>\n",
    "Fig 10: LSTM memory cell with a forget gate as  described by Gers et al. 2000\n",
    "\n",
    "$$\n",
    "\\mathbf{g}^{(t)} = \\phi (W^{gx} \\mathbf{x}^{(t)} + W^{gh} \\mathbf{h}^{(t-1)}) \\\\\n",
    "\\mathbf{i}^{(t)} = \\sigma (W^{ix} \\mathbf{x}^{(t)} + W^{ih} \\mathbf{h}^{(t-1)}) \\\\\n",
    "\\mathbf{f}^{(t)} = \\sigma (W^{fx} \\mathbf{x}^{(t)} + W^{fh} \\mathbf{h}^{(t-1)}) \\\\\n",
    "\\mathbf{o}^{(t)} = \\sigma (W^{ox} \\mathbf{x}^{(t)} + W^{oh} \\mathbf{h}^{(t-1)}) \\\\\n",
    "\\mathbf{s}^{(t)} = \\mathbf{g}^{(t)} \\otimes \\mathbf{i}^{(t)} + \\mathbf{s}^{(t-1)} \\otimes \\mathbf{f}^{(t)}\\\\\n",
    "\\mathbf{h}^{(t)} = \\phi (\\mathbf{s}^{(t)}) \\otimes \\mathbf{o}^{(t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A verbose discription\n",
    "<img src=LSTM_verbose.png width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=gates.png width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRNN \n",
    "<img src=BRNN.png width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (Skip-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram\n",
    "$P(w) = \\frac{number\\ of\\ appearance\\ of\\ w}{total\\ number\\ of\\ words}$\n",
    "### Skip-gram model\n",
    "* w: word\n",
    "* c: context (word that appeared around w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "<img src=http://web.mit.edu/amarbles/www/docs/word2vec_PCA.png width=800px>\n",
    "Retreived from [Here](http://web.mit.edu/amarbles/www/neuro_word2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code retrived  from [Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py) (and slightly modified). For more information, see [Here](https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html#highlights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここは別ノートに移したほうがいいかも知らん..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Translation\n",
    "### // BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://qiita-image-store.s3.amazonaws.com/0/100698/949b1930-5db2-008c-4db1-ad24b2eea4fe.png width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\begin{align}\n",
    "{\\bf i}_n & = \\tanh \\bigl( W_{xi} \\cdot {\\bf x}_n \\bigr), \\\\\n",
    "{\\bf p}_n & = {\\rm LSTM} \\bigl( W_{ip} \\cdot {\\bf i}_n + W_{pp} \\cdot {\\bf p}_{n-1} \\bigr), \\\\\n",
    "{\\bf q}_1 & = {\\rm LSTM} \\bigl( W_{pq} \\cdot {\\bf p}_{|{\\bf w}|} \\bigr), \\\\\n",
    "{\\bf q}_m & = {\\rm LSTM} \\bigl( W_{yq} \\cdot {\\bf y}_{m-1} + W_{qq} \\cdot {\\bf q}_{m-1} \\bigr), \\\\\n",
    "{\\bf j}_m & = \\tanh \\bigl( W_{qj} \\cdot {\\bf q}_m \\bigr), \\\\\n",
    "{\\bf y}_m & = {\\rm softmax} \\bigl( W_{jy} \\cdot {\\bf j}_m \\bigr).\n",
    "\\end{align}\n",
    "}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieved from [Here](http://qiita.com/odashi_t/items/a1be7c4964fbea6a116e). You can see Chainer implementation (Japanese)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Capturing\n",
    "Basically, That's translation from image to language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<img src=https://qiita-image-store.s3.amazonaws.com/0/100698/949b1930-5db2-008c-4db1-ad24b2eea4fe.png width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "* [Tensorflow Tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/index.html). Awesome\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Goldberg 2014](http://arxiv.org/abs/1402.3722)\n",
    "    * This note is an attempt to explain equation (4) (negative sampling) in \"Distributed Representations of Words and Phrases and their Compositionality\" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.\n",
    "* [Vinyals 2014](http://arxiv.org/abs/1411.4555)\n",
    "    * Show and Tell: A Neural Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献\n",
    "* [わかるLSTM](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)\n",
    "* [岩波データサイエンス2](https://sites.google.com/site/iwanamidatascience/vol2/natural-language-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
